{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "908e6456",
   "metadata": {},
   "source": [
    "# THIS IS THE EXAMPLE/DEMO FOR THE VECTOR/TOKEN EMBEDDING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762bfceb",
   "metadata": {},
   "source": [
    "### Import the gensim "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e38158",
   "metadata": {},
   "source": [
    "<div class= \"alert alert-block alert-info\">\n",
    "\n",
    "First we will donwload the gensim for further process \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e4c8bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting gensim\n",
      "  Downloading gensim-4.4.0-cp313-cp313-win_amd64.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\praja\\appdata\\roaming\\python\\python313\\site-packages (from gensim) (2.2.4)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\praja\\appdata\\roaming\\python\\python313\\site-packages (from gensim) (1.15.2)\n",
      "Collecting smart_open>=1.8.1 (from gensim)\n",
      "  Downloading smart_open-7.5.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting wrapt (from smart_open>=1.8.1->gensim)\n",
      "  Downloading wrapt-2.0.1-cp313-cp313-win_amd64.whl.metadata (9.2 kB)\n",
      "Downloading gensim-4.4.0-cp313-cp313-win_amd64.whl (24.4 MB)\n",
      "   ---------------------------------------- 0.0/24.4 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.5/24.4 MB 4.8 MB/s eta 0:00:05\n",
      "   - -------------------------------------- 0.8/24.4 MB 3.8 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 1.6/24.4 MB 2.8 MB/s eta 0:00:09\n",
      "   --- ------------------------------------ 1.8/24.4 MB 2.6 MB/s eta 0:00:09\n",
      "   ----- ---------------------------------- 3.1/24.4 MB 3.2 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 4.2/24.4 MB 3.6 MB/s eta 0:00:06\n",
      "   ------- -------------------------------- 4.5/24.4 MB 3.5 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 5.2/24.4 MB 3.4 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 5.5/24.4 MB 3.3 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 6.3/24.4 MB 3.1 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 6.8/24.4 MB 3.1 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 7.6/24.4 MB 3.1 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 8.4/24.4 MB 3.2 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 8.4/24.4 MB 3.2 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 8.4/24.4 MB 3.2 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 9.2/24.4 MB 2.8 MB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 10.0/24.4 MB 2.9 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 10.5/24.4 MB 2.9 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 11.0/24.4 MB 2.9 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 12.1/24.4 MB 2.9 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 13.1/24.4 MB 3.0 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 13.1/24.4 MB 3.0 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 13.6/24.4 MB 3.0 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 14.7/24.4 MB 3.0 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 15.5/24.4 MB 3.0 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 16.5/24.4 MB 3.1 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 17.6/24.4 MB 3.2 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 18.6/24.4 MB 3.2 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 19.1/24.4 MB 3.2 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 19.9/24.4 MB 3.2 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 21.0/24.4 MB 3.3 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 21.8/24.4 MB 3.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 22.5/24.4 MB 3.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.6/24.4 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.4/24.4 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 24.4/24.4 MB 3.3 MB/s  0:00:07\n",
      "Downloading smart_open-7.5.0-py3-none-any.whl (63 kB)\n",
      "Downloading wrapt-2.0.1-cp313-cp313-win_amd64.whl (60 kB)\n",
      "Installing collected packages: wrapt, smart_open, gensim\n",
      "\n",
      "   ---------------------------------------- 0/3 [wrapt]\n",
      "   ------------- -------------------------- 1/3 [smart_open]\n",
      "   ------------- -------------------------- 1/3 [smart_open]\n",
      "   ------------- -------------------------- 1/3 [smart_open]\n",
      "   ------------- -------------------------- 1/3 [smart_open]\n",
      "   -------------------------- ------------- 2/3 [gensim]\n",
      "   -------------------------- ------------- 2/3 [gensim]\n",
      "   -------------------------- ------------- 2/3 [gensim]\n",
      "   -------------------------- ------------- 2/3 [gensim]\n",
      "   -------------------------- ------------- 2/3 [gensim]\n",
      "   -------------------------- ------------- 2/3 [gensim]\n",
      "   -------------------------- ------------- 2/3 [gensim]\n",
      "   -------------------------- ------------- 2/3 [gensim]\n",
      "   -------------------------- ------------- 2/3 [gensim]\n",
      "   -------------------------- ------------- 2/3 [gensim]\n",
      "   -------------------------- ------------- 2/3 [gensim]\n",
      "   -------------------------- ------------- 2/3 [gensim]\n",
      "   -------------------------- ------------- 2/3 [gensim]\n",
      "   -------------------------- ------------- 2/3 [gensim]\n",
      "   -------------------------- ------------- 2/3 [gensim]\n",
      "   -------------------------- ------------- 2/3 [gensim]\n",
      "   -------------------------- ------------- 2/3 [gensim]\n",
      "   -------------------------- ------------- 2/3 [gensim]\n",
      "   -------------------------- ------------- 2/3 [gensim]\n",
      "   -------------------------- ------------- 2/3 [gensim]\n",
      "   -------------------------- ------------- 2/3 [gensim]\n",
      "   -------------------------- ------------- 2/3 [gensim]\n",
      "   -------------------------- ------------- 2/3 [gensim]\n",
      "   -------------------------- ------------- 2/3 [gensim]\n",
      "   -------------------------- ------------- 2/3 [gensim]\n",
      "   -------------------------- ------------- 2/3 [gensim]\n",
      "   ---------------------------------------- 3/3 [gensim]\n",
      "\n",
      "Successfully installed gensim-4.4.0 smart_open-7.5.0 wrapt-2.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ccdd09",
   "metadata": {},
   "source": [
    "### Import pretrained library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f20882c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api \n",
    "model = api.load(\"word2vec-google-news-300\") # Load pre-trained Word2Vec model # download the model and return as object ready to use \n",
    "# Link for above word2vec-google-news-300 for more details and explaination:- https://huggingface.co/fse/word2vec-google-news-300"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc06f956",
   "metadata": {},
   "source": [
    "<div class= \"alert alert-block alert-success\">\n",
    "\n",
    "Word2Vec:- \n",
    "\n",
    "Pre-trained vectors trained on a part of the Google News dataset (about 100 billion words). The model contains 300-dimensional vectors for 3 million words and phrases. The phrases were obtained using a simple data-driven approach described in 'Distributed Representations of Words and Phrases and their Compositionality'\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f86a2a9",
   "metadata": {},
   "source": [
    "### Example of a word as a vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb0f9456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.07421875e-01 -2.01171875e-01  1.23046875e-01  2.11914062e-01\n",
      " -9.13085938e-02  2.16796875e-01 -1.31835938e-01  8.30078125e-02\n",
      "  2.02148438e-01  4.78515625e-02  3.66210938e-02 -2.45361328e-02\n",
      "  2.39257812e-02 -1.60156250e-01 -2.61230469e-02  9.71679688e-02\n",
      " -6.34765625e-02  1.84570312e-01  1.70898438e-01 -1.63085938e-01\n",
      " -1.09375000e-01  1.49414062e-01 -4.65393066e-04  9.61914062e-02\n",
      "  1.68945312e-01  2.60925293e-03  8.93554688e-02  6.49414062e-02\n",
      "  3.56445312e-02 -6.93359375e-02 -1.46484375e-01 -1.21093750e-01\n",
      " -2.27539062e-01  2.45361328e-02 -1.24511719e-01 -3.18359375e-01\n",
      " -2.20703125e-01  1.30859375e-01  3.66210938e-02 -3.63769531e-02\n",
      " -1.13281250e-01  1.95312500e-01  9.76562500e-02  1.26953125e-01\n",
      "  6.59179688e-02  6.93359375e-02  1.02539062e-02  1.75781250e-01\n",
      " -1.68945312e-01  1.21307373e-03 -2.98828125e-01 -1.15234375e-01\n",
      "  5.66406250e-02 -1.77734375e-01 -2.08984375e-01  1.76757812e-01\n",
      "  2.38037109e-02 -2.57812500e-01 -4.46777344e-02  1.88476562e-01\n",
      "  5.51757812e-02  5.02929688e-02 -1.06933594e-01  1.89453125e-01\n",
      " -1.16210938e-01  8.49609375e-02 -1.71875000e-01  2.45117188e-01\n",
      " -1.73828125e-01 -8.30078125e-03  4.56542969e-02 -1.61132812e-02\n",
      "  1.86523438e-01 -6.05468750e-02 -4.17480469e-02  1.82617188e-01\n",
      "  2.20703125e-01 -1.22558594e-01 -2.55126953e-02 -3.08593750e-01\n",
      "  9.13085938e-02  1.60156250e-01  1.70898438e-01  1.19628906e-01\n",
      "  7.08007812e-02 -2.64892578e-02 -3.08837891e-02  4.06250000e-01\n",
      " -1.01562500e-01  5.71289062e-02 -7.26318359e-03 -9.17968750e-02\n",
      " -1.50390625e-01 -2.55859375e-01  2.16796875e-01 -3.63769531e-02\n",
      "  2.24609375e-01  8.00781250e-02  1.56250000e-01  5.27343750e-02\n",
      "  1.50390625e-01 -1.14746094e-01 -8.64257812e-02  1.19140625e-01\n",
      " -7.17773438e-02  2.73437500e-01 -1.64062500e-01  7.29370117e-03\n",
      "  4.21875000e-01 -1.12792969e-01 -1.35742188e-01 -1.31835938e-01\n",
      " -1.37695312e-01 -7.66601562e-02  6.25000000e-02  4.98046875e-02\n",
      " -1.91406250e-01 -6.03027344e-02  2.27539062e-01  5.88378906e-02\n",
      " -3.24218750e-01  5.41992188e-02 -1.35742188e-01  8.17871094e-03\n",
      " -5.24902344e-02 -1.74713135e-03 -9.81445312e-02 -2.86865234e-02\n",
      "  3.61328125e-02  2.15820312e-01  5.98144531e-02 -3.08593750e-01\n",
      " -2.27539062e-01  2.61718750e-01  9.86328125e-02 -5.07812500e-02\n",
      "  1.78222656e-02  1.31835938e-01 -5.35156250e-01 -1.81640625e-01\n",
      "  1.38671875e-01 -3.10546875e-01 -9.71679688e-02  1.31835938e-01\n",
      " -1.16210938e-01  7.03125000e-02  2.85156250e-01  3.51562500e-02\n",
      " -1.01562500e-01 -3.75976562e-02  1.41601562e-01  1.42578125e-01\n",
      " -5.68847656e-02  2.65625000e-01 -2.09960938e-01  9.64355469e-03\n",
      " -6.68945312e-02 -4.83398438e-02 -6.10351562e-02  2.45117188e-01\n",
      " -9.66796875e-02  1.78222656e-02 -1.27929688e-01 -4.78515625e-02\n",
      " -7.26318359e-03  1.79687500e-01  2.78320312e-02 -2.10937500e-01\n",
      " -1.43554688e-01 -1.27929688e-01  1.73339844e-02 -3.60107422e-03\n",
      " -2.04101562e-01  3.63159180e-03 -1.19628906e-01 -6.15234375e-02\n",
      "  5.93261719e-02 -3.23486328e-03 -1.70898438e-01 -3.14941406e-02\n",
      " -8.88671875e-02 -2.89062500e-01  3.44238281e-02 -1.87500000e-01\n",
      "  2.94921875e-01  1.58203125e-01 -1.19628906e-01  7.61718750e-02\n",
      "  6.39648438e-02 -4.68750000e-02 -6.83593750e-02  1.21459961e-02\n",
      " -1.44531250e-01  4.54101562e-02  3.68652344e-02  3.88671875e-01\n",
      "  1.45507812e-01 -2.55859375e-01 -4.46777344e-02 -1.33789062e-01\n",
      " -1.38671875e-01  6.59179688e-02  1.37695312e-01  1.14746094e-01\n",
      "  2.03125000e-01 -4.78515625e-02  1.80664062e-02 -8.54492188e-02\n",
      " -2.48046875e-01 -3.39843750e-01 -2.83203125e-02  1.05468750e-01\n",
      " -2.14843750e-01 -8.74023438e-02  7.12890625e-02  1.87500000e-01\n",
      " -1.12304688e-01  2.73437500e-01 -3.26171875e-01 -1.77734375e-01\n",
      " -4.24804688e-02 -2.69531250e-01  6.64062500e-02 -6.88476562e-02\n",
      " -1.99218750e-01 -7.03125000e-02 -2.43164062e-01 -3.66210938e-02\n",
      " -7.37304688e-02 -1.77734375e-01  9.17968750e-02 -1.25000000e-01\n",
      " -1.65039062e-01 -3.57421875e-01 -2.85156250e-01 -1.66992188e-01\n",
      "  1.97265625e-01 -1.53320312e-01  2.31933594e-02  2.06054688e-01\n",
      "  1.80664062e-01 -2.74658203e-02 -1.92382812e-01 -9.61914062e-02\n",
      " -1.06811523e-02 -4.73632812e-02  6.54296875e-02 -1.25732422e-02\n",
      "  1.78222656e-02 -8.00781250e-02 -2.59765625e-01  9.37500000e-02\n",
      " -7.81250000e-02  4.68750000e-02 -2.22167969e-02  1.86767578e-02\n",
      "  3.11279297e-02  1.04980469e-02 -1.69921875e-01  2.58789062e-02\n",
      " -3.41796875e-02 -1.44042969e-02 -5.46875000e-02 -8.78906250e-02\n",
      "  1.96838379e-03  2.23632812e-01 -1.36718750e-01  1.75781250e-01\n",
      " -1.63085938e-01  1.87500000e-01  3.44238281e-02 -5.63964844e-02\n",
      " -2.27689743e-05  4.27246094e-02  5.81054688e-02 -1.07910156e-01\n",
      " -3.88183594e-02 -2.69531250e-01  3.34472656e-02  9.81445312e-02\n",
      "  5.63964844e-02  2.23632812e-01 -5.49316406e-02  1.46484375e-01\n",
      "  5.93261719e-02 -2.19726562e-01  6.39648438e-02  1.66015625e-02\n",
      "  4.56542969e-02  3.26171875e-01 -3.80859375e-01  1.70898438e-01\n",
      "  5.66406250e-02 -1.04492188e-01  1.38671875e-01 -1.57226562e-01\n",
      "  3.23486328e-03 -4.80957031e-02 -2.48046875e-01 -6.20117188e-02]\n"
     ]
    }
   ],
   "source": [
    "word_vectors = model \n",
    "\n",
    "# Let us look how the vector embedding of word looks like \n",
    "print(word_vector['computer']) # Example: Accessing the vector for the word 'computer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0cc6b2c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "print(word_vectors['cat'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289cb6dc",
   "metadata": {},
   "source": [
    "# Similiar words "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afb249c",
   "metadata": {},
   "source": [
    "### King + Woman - Man = ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "368917b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('queen', 0.7118193507194519), ('monarch', 0.6189674139022827), ('princess', 0.5902431011199951), ('crown_prince', 0.5499460697174072), ('prince', 0.5377321839332581), ('kings', 0.5236844420433044), ('Queen_Consort', 0.5235945582389832), ('queens', 0.518113374710083), ('sultan', 0.5098593235015869), ('monarchy', 0.5087411403656006)]\n"
     ]
    }
   ],
   "source": [
    "# Example of using the most_simliar \n",
    "print(model.most_similar(positive=['king', 'woman'], negative=['man'], topn=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f173960",
   "metadata": {},
   "source": [
    "<div class= \"alert alert-block alert-success\">\n",
    "\n",
    "As you can see in the above cell that it captured the semantic meaning of the words as example given above if we add the king and woman and subtract the man from the equation it will give the output given the output cell the authority figure + female will give the outputs as same \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f6abc4",
   "metadata": {},
   "source": [
    "### Let us check the similarity between a few pair of words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b5187cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.76640123\n",
      "0.6510956\n",
      "0.7643474\n",
      "0.8543272\n",
      "0.7594368\n",
      "0.114080824\n"
     ]
    }
   ],
   "source": [
    "# Example of calculating similarity\n",
    "print(word_vectors.similarity('woman', 'man'))\n",
    "print(word_vectors.similarity('king', 'queen'))\n",
    "print(word_vectors.similarity('uncle', 'aunt'))\n",
    "print(word_vectors.similarity('boy', 'girl'))\n",
    "print(word_vectors.similarity('nephew', 'niece'))\n",
    "print(word_vectors.similarity('paper', 'water'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc812052",
   "metadata": {},
   "source": [
    "<div class= \"alert alert-block alert-success\">\n",
    "\n",
    "The above cell is for checking the similarity between the words as you can see we have used the similiar words for checking the similarity and it shows the great result for checking the similarity and it also gave the less score for the opposite for the example paper and water \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d1894b",
   "metadata": {},
   "source": [
    "### Most similar words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ccb3ecce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('towers', 0.8531750440597534), ('skyscraper', 0.6417425870895386), ('Tower', 0.639177143573761), ('spire', 0.594687819480896), ('responded_Understood_Atlasjet', 0.5931612253189087)]\n"
     ]
    }
   ],
   "source": [
    "print(word_vectors.most_similar(\"tower\", topn=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c062661",
   "metadata": {},
   "source": [
    "<div class= \"alert alert-block alert-success\">\n",
    "\n",
    "The above cell is for giving the words which have same meaning for the tower and it gave a good result for suggesting the similar words which have same or close meaning to tower  \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13aa3488",
   "metadata": {},
   "source": [
    "### Now let us see the vector similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f68f2de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Magnitude of difference between 'man' and 'woman': 1.7279510498046875\n",
      "Magnitude of difference between 'semiconductor' and 'earthworm': 5.6670427322387695\n",
      "Magnitude of difference between 'nephew' and 'niece': 1.9557794332504272\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "# Words to compare \n",
    "\n",
    "word1 = 'man'\n",
    "word2 = 'woman'\n",
    "\n",
    "word3 = 'semiconductor'\n",
    "word4 = 'earthworm'\n",
    "\n",
    "word5 = 'nephew'\n",
    "word6 = 'niece'\n",
    "\n",
    "# Calculate the vector difference \n",
    "vector_difference1 = word_vectors[word1] - word_vectors[word2]\n",
    "vector_difference2 = word_vectors[word3] - word_vectors[word4]\n",
    "vector_difference3 = word_vectors[word5] - word_vectors[word6]\n",
    "\n",
    "# Calculate the magnitude of the vector difference \n",
    "magnitude_of_difference1 = np.linalg.norm(vector_difference1)\n",
    "magnitude_of_difference2 = np.linalg.norm(vector_difference2)\n",
    "magnitude_of_difference3 = np.linalg.norm(vector_difference3)\n",
    "\n",
    "# Print the magnitude of the difference \n",
    "print(f\"Magnitude of difference between '{word1}' and '{word2}': {magnitude_of_difference1}\")\n",
    "print(f\"Magnitude of difference between '{word3}' and '{word4}': {magnitude_of_difference2}\")\n",
    "print(f\"Magnitude of difference between '{word5}' and '{word6}': {magnitude_of_difference3}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
